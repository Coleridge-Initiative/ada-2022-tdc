{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><br><br>\n",
    "    <h4>TANF Data Collaborative </h4>\n",
    "    <h4>Applied Data Analytics Training | Spring 2022</h4>\n",
    "    <h1>Characterizing Demand: Cluster Analysis</h1>\n",
    "</center>\n",
    "<center>\n",
    "    <span style=\"font-size: 1.5em;\">\n",
    "        <a href='https://www.coleridgeinitiative.org'>Coleridge Initiative</a>\n",
    "    </span>\n",
    "    <center>Maryah Garner, Benjamin Feder, Josh Edelmann, Nathan Barrett, Rukhshan Arif Mian</center>\n",
    "    <a href=\"https://doi.org/10.5281/zenodo.7459613\"><img src=\"https://zenodo.org/badge/DOI/10.5281/zenodo.7459613.svg\" alt=\"DOI\"></a>\n",
    "\n",
    "</center>\n",
    "\n",
    "<br>\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we do not have a clear outcome variable but nevertheless want to explore and discover any inherent groupings or configurations in the data. Unsupervised machine learning methods can help tackle these problems. Clustering is the most common unsupervised machine learning technique, but you might also be aware of principal components analysis (PCA) or neural networks implementations such as self-organizing maps (SOM). This notebook will provide an introduction to unsupervised machine learning through a clustering example.\n",
    "\n",
    "In this example, we will try to identify patterns within Indiana's labor market – namely patterns in the types of employers. Can we isolate specific employers based on derived features addressing their employees' experiences as measured by stability, opportunity, quality, and firm characteristics? We hope to do so using an unsupervised machine learning algorithm. Once we cluster Indiana's employers, we can explore how TANF recpients in the 2018 calendar year fit within the scope of Indiana's labor market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will focus on **K-Means clustering** (*k* defines the number of clusters), which is considered to be the most commonly-used clustering method. The algorithm works as follows:\n",
    "1. Select *k* (the number of clusters you want to generate).\n",
    "2. Initialize by selecting k points as centroids of the *k* clusters. This is typically done by selecting k points uniformly at random.\n",
    "3. Assign each point a cluster according to the nearest centroid.\n",
    "4. Recalculate cluster centroids based on the assignment in **(3)** as the mean of all data points belonging to that cluster.\n",
    "5. Repeat **(3)** and **(4)** until convergence. Convergence will be further discussed in the sections below.\n",
    "\n",
    "Please reference Chapter 7 of the Big Data and Social Science Textbook and the accompanying class videos for more information on unsupervised machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how *k*-means clustering can be used to better understand Indiana's labor market in 2018. We've already developed a handful of employer measures in a supplemental notebook (`Supplemental_Employer_Measures.ipynb`). We will experiment with a few different values of *k* to see how we can best understand the labor market by looking for differentiation between each of the clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages and Set Up\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main R package that we will use for clustering is called `cluster`. We also import all our usual packages for database connection and data manipulation/visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options(warn=-1)\n",
    "# Database interaction imports\n",
    "suppressMessages(library(odbc))\n",
    "\n",
    "# For data manipulation/visualization\n",
    "suppressMessages(library(tidyverse))\n",
    "\n",
    "# For faster date conversions\n",
    "suppressMessages(library(lubridate))\n",
    "\n",
    "# Use percent() function\n",
    "suppressMessages(library(scales))\n",
    "\n",
    "# clustering\n",
    "suppressMessages(library(cluster))\n",
    "options(warn=0)\n",
    "\n",
    "# set seed to ensure work is reproducible because k-means has random starting points\n",
    "set.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the server\n",
    "con <- DBI::dbConnect(odbc::odbc(),\n",
    "                     Driver = \"SQL Server\",\n",
    "                     Server = \"msssql01.c7bdq4o2yhxo.us-gov-west-1.rds.amazonaws.com\",\n",
    "                     Trusted_Connection = \"True\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code adjusting overall graph attributes\n",
    "\n",
    "# For easier reading, increase base font size\n",
    "theme_set(theme_gray(base_size = 16))\n",
    "# Adjust repr.plot.width and repr.plot.height to change the size of graphs\n",
    "options(repr.plot.width = 12, repr.plot.height = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read in the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will read in a table `employer_yearly_agg`, which contains characteristics of Indiana's labor market from 2005-2020, and limit it to information solely from 2018. This nearly mimics the years included in the employment outcomes analyses in the data exploration and longitudinal analysis notebooks. For more information as to how this table was created, please refer to the Supplemental notebook [\"Supplemental_Employer_Measures.ipynb.\"](Supplemental/Supplemental_Employer_Measures.ipynb)\n",
    "\n",
    "> Note: It is possible to cluster on all employers in all years at the same time as well—just keep in mind that there are a different set of assumptions that are prevalent with that decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`employer_yearly_agg` contains variables tracking the following characteristics on a quarterly basis, with the average (`avg_`) quarterly values reported within each year. The measures can be broken up within four separate categories and we go over these in the `04_Characterizing_Demand_Beginner.ipynb` notebook.\n",
    "\n",
    "We read in the aggregated employer data for 2018 using the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read aggregated employer data for 2018\n",
    "qry <- \"\n",
    "select *\n",
    "from tr_tdc_2022.dbo.employer_yearly_agg\n",
    "where year = 2018\n",
    "\"\n",
    "\n",
    "emp <- dbGetQuery(con, qry)\n",
    "\n",
    "# see employers\n",
    "head(emp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we sort the data in ascending order (based on all columns). SQL may read in the data in a different order every time we re-run our code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do.call constructs and executes a function call from a name or a function and a list of arguments to be passed to it.\n",
    "# we call the order function on all of emp's columns\n",
    "emp <- emp[do.call(order, emp), ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see all available columns\n",
    "names(emp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clean the Data\n",
    "\n",
    "A dataset must contain specific attributes in order to run a k-means clustering algorithm. We will examine `emp` through three steps:\n",
    "\n",
    "1. Remove non-explanatory and non-continuous features\n",
    "2. Examine scales across variables\n",
    "3. Analyze missingness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Non-Explanatory and Non-Continuous Features\n",
    "\n",
    "For any algorithm, non-explanatory variables such as unique ids should be removed from the data set. Additionally, k-means algorithms only work properly with continuous features. This is because k-means calculates its distance measure using euclidean distance, which is the distance between each data point and the centroid of a cluster. It is difficult to assign positions for categorical variables in the euclidean space.\n",
    "\n",
    "> There are more sophisticated clustering algorithms that do not use Euclidean distances and thus allow categorical variables in the model. If you are interested in them, you can take a look at the functions `kmodes` and `gower.dist` in R - you will need to download their respective libraries first.\n",
    "\n",
    "In this case, we select out the following columns:\n",
    "\n",
    "- Empr_no: Employer Number\n",
    "- year: Year \n",
    "- naics_code: Employer's naics code\n",
    "- adj_naics_2: Employer's 2 digit naics code\n",
    "\n",
    "We do this using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove features without explanatory power\n",
    "emp_ml <- emp %>%\n",
    "    select(-c(Empr_no, year, naics_code, adj_naics_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine Scales Across Variables\n",
    "\n",
    "Next, we use the `str` function to see if there are any categorical variables that remain in our data frame (now called `emp_ml`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data type of all variables - make sure all of them are numeric\n",
    "str(emp_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the different explanatory metrics are on a variety of numerical scales, the k-means algorithm will overweigh variables on larger scales due to its distance metric. For this purpose, we first convert all variables to a **numeric** type and scale them.\n",
    "\n",
    "Note that we do see variables with the type **int** and **integer64** – we are going to convert these to a **numeric** (or **num**) type as well as integers do not scale in R. \n",
    "\n",
    "We utilize the `sapply` function to do this conversion. It takes a data frame (or a list/vector) as input and gives output in a vector or matrix. It allows us to batch convert all columns in our data frame to a numeric type. We use the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all numeric variables to numeric type otherwise integer64 won't scale using sapply\n",
    "emp_ml_num <- emp_ml %>%\n",
    "    sapply(as.numeric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the `scale` function to scale all columns in our data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features since variables like avg_emp_rate are much smaller than avg_total_earnings\n",
    "emp_ml_scale <- scale(emp_ml_num)\n",
    "\n",
    "# View first rows after scaling\n",
    "emp_ml_scale %>% \n",
    "   head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dim(emp_ml_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Missingness\n",
    "\n",
    "If an employer has missing information in any of the columns, the row will be dropped in the clustering method.\n",
    "\n",
    "> Note that you should **never remove data** if possible - in a real world setting you would likely want to fill any missing data with an imputation method or a baseline assumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of rows (where each row is a unique employer/year combination)\n",
    "nrow(emp_ml_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# na.omit will remove any rows with any NA values\n",
    "emp_ml_scale <- na.omit(emp_ml_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check number of rows after dropping rows with any NA values\n",
    "# see that there is no missing data\n",
    "nrow(emp_ml_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using `na_omit` to remove missing values in our data frame of interest (`emp_ml_scale`), we see that our number of observations does not change – this means that the dataframe has no missing values in the columns of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Choose the Number of Clusters, *K*\n",
    "\n",
    "Running a *k*-means model is simple: we just need to use `kmeans()` and choose the number of clusters (called `centers`). What number should we choose? Here, we have a bunch of features, so it can be difficult to visualize the data in order to choose the proper number.\n",
    "\n",
    "Luckily, there are a few procedures we can use to help us find an appropriate *k* value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One technique that can be used to help determine *k* in the elbow method. In the elbow method, we try different k values and calculate the sum of squared errors (`SSE`) after the model converges. Then we plot all the `SSE` by K in a line-chart. The line-chart should resemble an arm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, we define a function called `wss`. It allows us to compute the total within-cluster sum of squares. \n",
    "\n",
    "Functions are useful when you want to perform a certain task multiple times. A function accepts input arguments and produces the output by executing valid R commands that are inside the function. \n",
    "\n",
    "Within this function, we will use `nstart = 1` to minimize the code runtime – we discuss this later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to compute total within-cluster sum of square\n",
    "# we can run this for multiple values of k – showcased later in this notebook\n",
    "wss <- function(k) {\n",
    "    kmeans(emp_ml_scale, centers=k, nstart=1)$tot.withinss\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define `k.values` that takes values between 1 and 15 (inclusive). We utilize `map_dbl` to run the `wss` function 15 times. That is, we run `wss` for `k=1`, `k=2` up until `k=15`. \n",
    "\n",
    "`map_dbl` allows us to do this in one line of code. Resulting values for each run are stored in `wss_values`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute and plot wss for k =1 to k = 15\n",
    "k.values <- 1:15\n",
    "\n",
    "# extract wss values for each k\n",
    "wss_values <- map_dbl(k.values, wss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have `wss_values`, we can plot these using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wss_df <- data.frame(wss_values, k.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting wss_df\n",
    "wss_df %>%\n",
    "    ggplot(aes(x=k.values, y=wss_values)) + \n",
    "    geom_line() + \n",
    "    geom_point()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We try to choose the number around the inflection point, where the change in SSE becomes negligible, indicating that there is little room to improve the model by increasing k (the bend in the elbow). On our graph, the elbow curve begins to flatten around k = 4.\n",
    "\n",
    "> Note: The jaggedness in the curve is due to the default `nstart` value being `1`. `nstart` specifies a number of initial configurations and reports on the best one. However, due to the size of the data, if you increase `nstart` in the function above, it may not converge based on the number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Model\n",
    "\n",
    "Now that we have chosen a value for **k** using the elbow method, we move forward and utilize this using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and run on emp_ml_scale with centers = 4\n",
    "k4 <- kmeans(emp_ml_scale, centers = 4, nstart = 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> An optimal number for `nstart` is usually somewhere between 20 and 50. (See more information in the Resources section - Professor Steorts, Duke University)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the `kmeans` function returns the following components, with the most useful for us now as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names(k4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where: \n",
    "\n",
    "- `cluster` - an integer indicating a cluster to which each point is allocated\n",
    "- `centers` - a matrix of cluster centers\n",
    "- `totss` - the total sum of squares\n",
    "- `withinss` - vector of within-cluster sum of squares, one component per cluster.\n",
    "- `tot.withinss` - total within-cluster sum of squares, i.e. `sum(withinss)`\n",
    "- `betweenss` - the between-cluster sum of squares, i.e. `totss-tot.withinss`\n",
    "- `size` - the number of points in each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the size of each cluster by using the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see size of cluster\n",
    "k4$size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that most of the employers are concentrated in cluster 3. In the perfect world, we would want them to be distributed more evenly across clusters, but in some cases, it may make sense that they wouldn't. \n",
    "\n",
    "We will continue to run our model through a battery of tests to further inform our clustering decision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Describe Features across Clusters\n",
    "\n",
    "To start to tease out the potential for high intra-cluster and low inter-cluster similarity, we can take a look at basic descriptives of the employers in these clusters. This will allow us to start to get a sense of some of the important differences across the clusters in the hopes of categorizing each cluster as a separate \"type\" of employer relative to those in other clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names(emp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_few_cols <- emp %>%\n",
    "    select(-c(naics_code, adj_naics_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove missing values (none here)\n",
    "emp_few_cols <- na.omit(emp_few_cols) \n",
    "\n",
    "dim(emp_few_cols)\n",
    "# add cluster number to the original dataframe\n",
    "frame_4 <- emp_few_cols %>% \n",
    "    mutate(k4.cluster = k4$cluster)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names(frame_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empr_nbr, year, and naics codes related columns\n",
    "frame_4_few_cols <- frame_4 %>%\n",
    "    select(-c(Empr_no, year))\n",
    "\n",
    "# summarize and add in sizes of each cluster\n",
    "frame_4_few_cols %>%\n",
    "    group_by(k4.cluster) %>%\n",
    "    # getting averages for each cluster\n",
    "    # add suffix \"by_employer\" to each summarize variable\n",
    "    summarise(across(everything(), # adds the suffix across every column in our dataframe\n",
    "                     list(by_employer=mean))) %>%\n",
    "    mutate(\n",
    "        size = k4$size\n",
    "    ) %>%\n",
    "    # relocates the size column after the k4.cluster columns\n",
    "    relocate(size, .after=k4.cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, we can see that our biggest cluster, cluster 2, contains relatively small employers with medium stability and opportunity but low job quality. Cluster 3 also has larger employers, and on average, they also have low job quality, low stability. In contrast, cluster 4 contains some of the larger employers in the state that tend to have better job quality while cluster 1 has midsize employers with good job quality and stability.\n",
    "\n",
    "> When clustering, be cognizant that small numbers of employers per cluster may result in additional redaction due to disclosure regulations concerning the number of employers contributing to each cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Key Variables of Interest\n",
    "\n",
    "We can also visualize the differences between in clusters in more detail by including the standard deviation in conjunction with the average for some of the differentiating features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# focus on avg_avg_earnings, avg_bottom_25_pctile, avg_top_75_pctile (all measures of job quality)\n",
    "# need to convert summarize data frame into a long format with each variable/cluster combination corresponding to a single row\n",
    "frame_4_mean <- frame_4_few_cols %>%\n",
    "    group_by(k4.cluster) %>%\n",
    "    select(k4.cluster, avg_avg_earnings, avg_bottom_25_pctile, avg_top_75_pctile) %>%\n",
    "    summarise(across(everything(), mean)) %>%\n",
    "    # reshaping the date frame from wide to long\n",
    "    # pivot_longer \"lengthens\" data, increasing the number of rows and decreasing the number of columns (from: https://tidyr.tidyverse.org/)\n",
    "    pivot_longer(-k4.cluster, names_to = \"variable\", values_to = \"mean\")\n",
    "\n",
    "# Save results with standard deviation to a dataframe\n",
    "frame_4_sd <- frame_4_few_cols %>%\n",
    "    group_by(k4.cluster) %>%\n",
    "    select(k4.cluster, avg_avg_earnings, avg_bottom_25_pctile, avg_top_75_pctile) %>%\n",
    "    summarise(across(everything(), sd)) %>%\n",
    "     # reshaping the date frame from wide to long\n",
    "    # pivot_longer \"lengthens\" data, increasing the number of rows and decreasing the number of columns (from: https://tidyr.tidyverse.org/)\n",
    "    pivot_longer(-k4.cluster, names_to = \"variable\", values_to = \"sd\") %>%\n",
    "    select(-c(k4.cluster, variable))\n",
    "\n",
    "# Bind two dataframes together\n",
    "df <- cbind(frame_4_mean,frame_4_sd)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_quality <- c('avg_avg_earnings', 'avg_bottom_25_pctile', 'avg_top_75_pctile')\n",
    "\n",
    "df$measure_cat <- \"Job Quality\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare mean + sd across all four clusters for variables of interest\n",
    "df %>%\n",
    "ggplot(aes(x=k4.cluster,\n",
    "           y=mean, \n",
    "           fill=k4.cluster)) +\n",
    "    geom_bar(stat=\"identity\", position = position_dodge()) +   # plot bars for the mean values\n",
    "    geom_errorbar(aes(ymax= mean + sd, ymin = mean),            # add standard deviation bars\n",
    "                  width=.2,\n",
    "                  position = position_dodge(.9)) +\n",
    "    facet_grid(. ~ variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can really see the differences in job quality across the clusters as well as where there may be potential overlap. There are additional steps you can take to further justify a clustering choice, but in clustering there may not be a single right answer - every time we run a different number of clusters, interesting patterns about our data may emerge. It may be useful to try running the algorithm on different numbers of clusters and comparing the outputs between models.\n",
    "\n",
    "We really want to know *whether the clusters that we find represent true subgroups in our data*. This could be a crucial input toward choosing the right number of clusters. (See more information on additional methods for selecting `k` in the Resources section - Professor Steorts, Duke University)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting column for 2 digit naics code\n",
    "emp_naics <- emp %>% \n",
    "    select(Empr_no, adj_naics_2)\n",
    "\n",
    "# Bringing in naics codes \n",
    "frame_4_naics <- inner_join(frame_4, emp_naics, by=\"Empr_no\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_4_naics %>%\n",
    "    group_by(k4.cluster, adj_naics_2) %>%\n",
    "    summarize(count_most_freq = n_distinct(Empr_no)) %>%\n",
    "    arrange(k4.cluster, desc(count_most_freq)) %>%\n",
    "    ungroup() %>%\n",
    "    group_by(k4.cluster) %>%\n",
    "    mutate(total = sum(count_most_freq)) %>%\n",
    "    filter(row_number()==1) %>%\n",
    "    mutate(perc = round(count_most_freq*100/total, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Relating Back to the Cohort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Armed with descriptions of employers existing within Indiana's labor force, we can now relate the results of our unsupervised machine learning model back to our cohort of interest.  In this section we will take a look at how our original cohort of 2018 TANF exiters fit within these clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read earnings of cohort into R subset to 2017 jobs\n",
    "qry <- \"\n",
    "select *, CAST(Year as VARCHAR) + ' Q' + CAST(Q as VARCHAR) as yr_quarter\n",
    "from tr_tdc_2022.dbo.nb_cohort_wages\n",
    "\"\n",
    "df_wages <- dbGetQuery(con, qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add cluster number to the original dataframe\n",
    "frame_4 <- emp %>%                     \n",
    "    mutate(k4.cluster = k4$cluster)  \n",
    "\n",
    "# Join wages table with frame_4 clustering results\n",
    "df_wages_clus <- df_wages %>%\n",
    "    inner_join(frame_4, by='Empr_no')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Employers who Employed a Member of the Cohort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see number of employers by cluster that employed someone in the cohort\n",
    "df_wages_clus %>%\n",
    "    group_by(k4.cluster) %>%\n",
    "    summarise(emp_cohort = n_distinct(Empr_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see number of employers by cluster that employed someone in the cohort\n",
    "df_wages_clus %>%\n",
    "    group_by(k4.cluster, yr_quarter) %>%\n",
    "    filter(yr_quarter != '2018 Q2', yr_quarter != '2019 Q3') %>%\n",
    "    summarise(emp_cohort = n_distinct(Empr_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare within cohort employers to all employers in original clusters\n",
    "# Get number of unique employers per cluster in the full dataframe (all employers)\n",
    "frame_4 %>%\n",
    "    group_by(k4.cluster) %>%\n",
    "    summarise(emp_all = n_distinct(Empr_no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare with percentages\n",
    "cohort_emp <- df_wages_clus %>%\n",
    "    group_by(k4.cluster) %>%\n",
    "    summarise(emp_cohort = n_distinct(Empr_no))\n",
    "\n",
    "emp_all <- frame_4 %>%\n",
    "    group_by(k4.cluster) %>%\n",
    "    summarise(emp_all = n_distinct(Empr_no))\n",
    "\n",
    "# Join cohort employers with all employers, andd find percentage\n",
    "cohort_emp %>%\n",
    "    inner_join(emp_all, by = 'k4.cluster') %>%\n",
    "    mutate(percentage = (emp_cohort / emp_all) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a reminder, one limitation of our original employers file is that it doesn't include quarterly information for employers with less than 5 employees in a specific quarter. We can see that outside of the employers in Cluster 2 (our smallest cluster by amount of employers), the majority of employers in other clusters did not employ anyone in our cohort in 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Wages over time\n",
    "\n",
    "Next, we look at the average wages for employees within each cluster over time. To do this, we first calculate the mean earnings for our cohort and the number of individuals for each cluster-quarter combination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average quarterly earnings and number individuals employers in at least one quarter for cohort by cluster\n",
    "df_avg_wages_clus <- df_wages_clus %>%\n",
    "    group_by(k4.cluster, yr_quarter) %>%\n",
    "    summarise(\n",
    "        mean_earnings_cohort = mean(Wage),\n",
    "        num_individuals = n_distinct(ssn)\n",
    "    )\n",
    "\n",
    "df_avg_wages_clus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we plot the average wages by cluster and quarter to understand outcomes for our cohort over time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_avg_wages_clus %>%\n",
    "    ggplot() + \n",
    "    aes(x = yr_quarter,\n",
    "        y = mean_earnings_cohort, group=k4.cluster, color=k4.cluster)  +  \n",
    "    geom_line(position='dodge',stat='identity') + \n",
    "    expand_limits(y= 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that among our 2018 TANF cohort, those who worked for employers in clusters 3 and 4 earned more, on average. We can see similar trends in average quarterly wages amongst all employees and those in our cohort acrosss the clusters, except that those in the clusters 1 and 2 made less on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "- UC Business Analytics R Programming Guide: https://uc-r.github.io/kmeans_clustering\n",
    "- Rebecca Steorts, Assistant Professor, Duke University, Department of Statistical Science, Data Mining and Machine Learning course: https://github.com/resteorts/data-mine/tree/master/lectures_2018/10-unsupervise/10-kmeans.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.3"
  },
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
